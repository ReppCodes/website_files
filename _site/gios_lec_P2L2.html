<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="generator" content="Jekyll">

  <title>GIOS Lecture Notes - Part 2 Lesson 2 - Threads and Concurrency</title>

  <link rel="stylesheet" href="/css/main.css">
  
</head>


<body>
  <div id="wrapper">
    <header>
  <div>
    <a href="/">
    
    <h1>andrew@theinternet</h1>
    </a>
    <div class="header-links">
      <a href="/OMSCS.html"><h2 class="header-link">OMSCS</h2></a>
<a href="/DS.html"><h2 class="header-link">Data Science</h2></a>

    </div>
  </div>
</header>
    <div class="container">
      <section id="main_content">
        <article>
  <h2>GIOS Lecture Notes - Part 2 Lesson 2 - Threads and Concurrency</h2>
  <hr />
<h2 id="why-threads">Why threads?</h2>
<p>Multiple processes from last lecture can still only happen on one CPU at a time.  To take advantgage of multiple CPUs we need multiple execution contexts.  This is usually done with threads.</p>

<h2 id="visual-metaphor-for-threads">Visual Metaphor for Threads</h2>
<p>Again this toy shop comparison.  This time the workers in the shop.</p>
<ul>
  <li>Active</li>
  <li>Works simultaneously</li>
  <li>Requires coordination</li>
</ul>

<h2 id="process-vs-thread">Process vs Thread</h2>
<ul>
  <li>A single threaded process is represented by its address space
    <ul>
      <li>Also by its execution context.  Represented in a PCB</li>
    </ul>
  </li>
  <li>Multiple threads all share the same address space, code, data, and files.  However, each execute different instructions and operate on different parts of space
    <ul>
      <li>As a solution to this, a more complex PCB structure is needed.  It will keep track of both the shared information and the per-thread execution context
<img src="../assets/content_images/omscs/gios/p2l2_img1.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h2 id="benefits-of-multithreading">Benefits of Multithreading</h2>
<h3 id="why-are-threads-useful">Why are threads useful?</h3>
<ul>
  <li>At any given time, there may be multiple threads each running on a different processor</li>
  <li>One potential application is that you can have many threads executing the same operation on a different input structure
    <ul>
      <li>This parallelization grants faster overall runtime</li>
    </ul>
  </li>
  <li>Alternatively, you may have threads execute different portions of the code that correspond to different functions
    <ul>
      <li>Specialization allows differentiation of how we manage the functions
        <ul>
          <li>Hot cache</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Threading has much lighter memory requirements, as it does not need separate allocations for address space information and execution contexts for each thread while multiprocessing does.</li>
  <li>Passing data among processes requires IPC between address spaces.  Much harder than threads.</li>
</ul>

<h2 id="benefits-of-multithreading-single-cpu">Benefits of Multithreading: Single CPU</h2>
<ul>
  <li>Are threads useful when #threads &gt; #cpus
    <ul>
      <li>Yes, take advantage if IO or other idle time</li>
      <li>Any time the idle is longer than 2x time of context switch you could be working another thread</li>
      <li>Notably context switches are much lighter for threads than processes</li>
      <li>This is known as “hiding latency”</li>
    </ul>
  </li>
</ul>

<h2 id="basic-thread-mechanisms">Basic Thread Mechanisms</h2>
<ul>
  <li>Thread data structure
    <ul>
      <li>Identify threads, keep track of resource usage</li>
    </ul>
  </li>
  <li>Mechanisms to create and manage threads</li>
  <li>Mechanisms to safely coordinate among threads running concurrently in the same address space
    <ul>
      <li>You can get data races or other issues when threads try to all access the same data concurrently</li>
    </ul>
  </li>
  <li>Concurrency control and coordination
    <ul>
      <li>Mutual exclusion
        <ul>
          <li>Exclusive access to only one thread at a time</li>
          <li>mutex - a synchronization mechanism</li>
        </ul>
      </li>
      <li>Waiting on other threads
        <ul>
          <li>specific condition before proceeding</li>
          <li>Condition variables - a synchronization mechanism</li>
        </ul>
      </li>
      <li>Waking up other threads from wait state</li>
    </ul>
  </li>
</ul>

<h2 id="threads-and-thread-creation">Threads and Thread Creation</h2>
<h4 id="this-section-based-on-birrell-paper-not-pthreads-library">THIS SECTION BASED ON BIRRELL PAPER, NOT PTHREADS LIBRARY</h4>
<ul>
  <li>Thread type
    <ul>
      <li>thread data structure</li>
      <li>contains all thread info to describe the thread</li>
    </ul>
  </li>
  <li>Fork (proc, args)
    <ul>
      <li>create a thread</li>
      <li>not UNIX fork()</li>
      <li>Need some mechanism to determine when forked thread is done, and retrieve its result</li>
    </ul>
  </li>
  <li>Join (thread)
    <ul>
      <li>terminate a thread</li>
      <li>returns to parent the result of the child’s executrion
<img src="../assets/content_images/omscs/gios/p2l2_img2.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h2 id="mutexes">Mutexes</h2>
<ul>
  <li>Mutal Exclusion</li>
  <li>A lock that should be used whenever accessing data or state that’s shared among threads</li>
  <li>To support this function, need a data structure for a mutes
    <ul>
      <li>Locked?</li>
      <li>Owner</li>
      <li>blocked_threads</li>
    </ul>
  </li>
  <li>Portion of code protected by the mutex is called the “critical sectioon”</li>
</ul>

<h2 id="producer--consumer-example">Producer / Consumer Example</h2>
<ul>
  <li>What if the processing you wish to perform with mutex needs to occur only under certain conditions?
<img src="../assets/content_images/omscs/gios/p2l2_img3.png" alt="" /></li>
</ul>

<h2 id="condition-variables">Condition Variables</h2>
<p><img src="../assets/content_images/omscs/gios/p2l2_img4.png" alt="" /></p>
<ul>
  <li>Condition type</li>
  <li>Wait(mutex. cond)
    <ul>
      <li>mutex automatically released &amp; re-aquired on wait</li>
    </ul>
  </li>
  <li>Signal(cond)
    <ul>
      <li>notify only one thread waiting on condition</li>
    </ul>
  </li>
  <li>Broadcast(cond)
    <ul>
      <li>notify all waiting threads</li>
      <li>worth noting that in this case there’s still a mutex to be acquired, only one thread woken will get it.  Use case for Broadcast() is somewhat complicated as a result</li>
    </ul>
  </li>
  <li>Condition Variable Data Structure
    <ul>
      <li>mutex ref</li>
      <li>waiting threads</li>
    </ul>
  </li>
</ul>

<h2 id="readerswriter-problem">Readers/Writer Problem</h2>
<p><img src="../assets/content_images/omscs/gios/p2l2_img5.png" alt="" />
<img src="../assets/content_images/omscs/gios/p2l2_img6.png" alt="" />
<img src="../assets/content_images/omscs/gios/p2l2_img7.png" alt="" /></p>

<h2 id="common-pitfalls">Common Pitfalls</h2>
<h3 id="avoiding-common-mistakes">Avoiding common mistakes</h3>
<ul>
  <li>Keep track of mutex/condvariables used with a aresource
    <ul>
      <li>e.g. mutex_type m1; //mutex for file</li>
    </ul>
  </li>
  <li>Check that you are always (and correctly) using lock and unlock
    <ul>
      <li>common mistake is to forget this in one of many places where a given piece of state is accessed</li>
      <li>e.g. did you forget to lock/unlock compiler errors</li>
    </ul>
  </li>
  <li>Use a single mutex to access a single resource</li>
  <li>Check that you are signaling correct condition</li>
  <li>Check that you are not using signal when broadcast is needed
    <ul>
      <li>signal: only 1 thread will proceed, remaining threads will continue to wait, possibly indefinitely</li>
    </ul>
  </li>
  <li>Ask yourself: do you need priority guarantees?
    <ul>
      <li>thred execution order not controlled by signals to condition variable</li>
    </ul>
  </li>
  <li>Spurious wake ups
    <ul>
      <li>Doesn’t necessarily affect correctness, but will affect performance</li>
      <li>When we wake threads up knowing they may not be able to proceed</li>
      <li>Can we unlock the mutex before broadcast/signal?
        <ul>
          <li>Yes! Sometimes…</li>
          <li>But you need to be very careful about ordering and atomicity</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Deadlocks
    <ul>
      <li>Two or more competing threads are waiting on each other to complete, but none of them ever do</li>
      <li>How to avoid?
        <ul>
          <li>Fine-grained locking</li>
          <li>Get all locks upfront then release at end</li>
          <li>Use one mega lock</li>
          <li>Too restrictive, often</li>
          <li>Maintain lock order!
            <ul>
              <li>Hierarchical locking.  This is generally better than above solutions</li>
              <li>In practice this can be very complicated on more complex systems</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>In summary
        <ul>
          <li>A cycle in the wait graph is necessary and sufficient for a deadlock to occur
            <ul>
              <li>Edges from thread waiting on a resource to thread owning a source</li>
            </ul>
          </li>
          <li>What can we do about it?
            <ul>
              <li>Deadlock prevention
                <ul>
                  <li>Hierarchical locking</li>
                  <li>EXPENSIVE</li>
                </ul>
              </li>
              <li>Deadlock detection and recovery
                <ul>
                  <li>Based on graph analysis</li>
                  <li>ROLLBACK - still expensive because you need to maintain enough state to recover
                    <ul>
                      <li>Recovery may not always be possible</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>Ostrich Algorithm
                <ul>
                  <li>Head in the sand yo</li>
                  <li>Reboot if it stops working</li>
                  <li>Obviously not a great idea</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="kernel-vs-user-level-threads">Kernel vs User Level Threads</h2>
<ul>
  <li>Kernel level threads imply that the OS itself is multi-threaded</li>
  <li>OS scheduler handles how kernel level threads will work</li>
</ul>

<h3 id="one-to-one-model">One to One Model</h3>
<ul>
  <li>Each user level thread has a kernel level thread associated with it</li>
  <li>Pros
    <ul>
      <li>OS sees and understands the threads, synchronization, blocker, etc</li>
      <li>User level processes directly benefit from threading in kernel</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>Must go to OS for all operations (may be expensive)</li>
      <li>OS may have limits on policies, thread number, etc</li>
      <li>Portability</li>
    </ul>
  </li>
</ul>

<h3 id="many-to-one-model">Many to One Model</h3>
<ul>
  <li>All user level threads are mapped onto single kernel level thread</li>
  <li>At user level there is a thread management library that decides which user level thread will be mapped onto the kernel level thread at any given time</li>
  <li>Pros
    <ul>
      <li>Totally portable</li>
      <li>Doesn’t depend on OS limits and policies</li>
      <li>Don’t have to make system calls or context switches, less overhead</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>OS has no insights into application needs</li>
      <li>OS may block entire process if one user-level thread blocks on I/O</li>
    </ul>
  </li>
</ul>

<h3 id="many-to-many-model">Many to Many Model</h3>
<ul>
  <li>Allows some user threads to be associated with one kernel level process, while others may be one to one</li>
  <li>Pros
    <ul>
      <li>Can be best of both worlds</li>
      <li>Can have bound or unbound threads</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>Requires coordination between user and kernel level thread managers</li>
    </ul>
  </li>
</ul>

<h2 id="scope-of-multithreading">Scope of Multithreading</h2>
<ul>
  <li>Different levels at which multithreading is supported</li>
  <li>System scope
    <ul>
      <li>system-wide thread management by OS-level thread managegers (e.g. CPU scheduler)</li>
    </ul>
  </li>
  <li>Process scope
    <ul>
      <li>User level library manages threads within a single process</li>
    </ul>
  </li>
</ul>

<h2 id="multithreading-patterns">Multithreading Patterns</h2>
<h3 id="boss-workers">Boss-workers</h3>
<ul>
  <li>Characterized by one boss thread and multiple worker threads</li>
  <li>Boss assigns work to workers</li>
  <li>Worker performs entire task</li>
  <li>Throughput of the system limited by boss thread.  Must keep boss thread efficient
    <ul>
      <li>Throughput = 1/(boss time per order)</li>
    </ul>
  </li>
  <li>How does boss pass work?
    <ul>
      <li>One way is for boss to keep track of which workers are free, and directly signal specific worker
        <ul>
          <li>Pros
            <ul>
              <li>Workers don’t need to synchronize</li>
            </ul>
          </li>
          <li>Cons
            <ul>
              <li>Boss must track what each worker is doing</li>
              <li>Boss must wait on targeted worker</li>
              <li>Throughput will go down</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Another option is for boss to assign work by placing work in producer/consumer queue that workers take from
        <ul>
          <li>Pros
            <ul>
              <li>Boss doesn’t need to know details about workers</li>
              <li>Boss doesn’t need to wait on workers</li>
              <li>Better throughput</li>
            </ul>
          </li>
          <li>Cons
            <ul>
              <li>Queue synchronization</li>
            </ul>
          </li>
          <li>This approach tends to be picked</li>
          <li>If queue is full, boss has to wait to add stuff to it.  More workers to pull from it will empty queue faster</li>
          <li>How many workers is optimal?
            <ul>
              <li>On demand, dynamically?</li>
              <li>Statically?
                <ul>
                  <li>More commonly, keep a pool of workers created up front</li>
                </ul>
              </li>
              <li>Static vs dynamic
                <ul>
                  <li>Increase pool in size of some conditions are met</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Pros
    <ul>
      <li>Simplicity</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>thread pool management</li>
      <li>ignores locality
        <ul>
          <li>boss doesn’t keep track of workers.</li>
          <li>If a worker just finished a specific task, more likely to be efficient to do it again.  State is loaded already.  You leave that potential optimization on the table here.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>An alternative to all workers performing all same task, you can have workers specialized for certain tasks
    <ul>
      <li>Boss would now need to classify tasks, and assign to correct type of worker.   This creates bottleneck overhead, but my be offset by worker efficiency gains</li>
      <li>better locality - because a worker does only a subset of tasks, more likely to have the needed state already loaded in cache</li>
      <li>Quality of Service management - can better prioritize specific tasks</li>
      <li>Downside is the overhead of more complex load balancing</li>
    </ul>
  </li>
</ul>

<h3 id="pipeline">Pipeline</h3>
<ul>
  <li>Threads assigned one subtask in the system</li>
  <li>Entire tasks == pipeline of threads</li>
  <li>Multiple tasks concurrently in the system, in different pipeline stages</li>
  <li>Throughput == weakest link (longest stage in the pipeline)</li>
  <li>Each pipeline stage has its own thread pool</li>
  <li>Shared-buffer based communication betweeen stages</li>
  <li>Pros
    <ul>
      <li>Allows for highly specialized threads, great locality</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>Balancing and synchronization overheads</li>
    </ul>
  </li>
</ul>

<h3 id="layered">Layered</h3>
<ul>
  <li>Each layer is assigned a group of related subtasks</li>
  <li>End to end task must pass up and down through all layers</li>
  <li>Pros
    <ul>
      <li>Specialization and locality</li>
      <li>Less fine-grained than pipeline</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>Not suitable for all applications</li>
      <li>Synchronization</li>
    </ul>
  </li>
</ul>

<h3 id="throughput-quiz">Throughput Quiz</h3>
<p>Boss-Worker Formula:
time_to_finish_1_order * ceiling (num_orders / num_concurrent_threads)</p>

<p>Pipeline Formula:
time_to_finish_first_order + (remaining_orders * time_to_finish_last_stage)</p>

</article>
      </section>
    </div>
  </div>

   
  
  <footer>
  <a href="https://creativecommons.org/licenses/by-nc/3.0/deed.en_US">
    <span>
        <b>Andrew Repp</b>
    </span>
    
    <span>© 2023</span>
  </a>
</footer>

</body>

</html>