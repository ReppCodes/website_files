<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <meta name="generator" content="Jekyll">

  <title>NetSec Lecture Notes - Lesson 15 - Internet-Scale Threat Analysis -- Scanning</title>

  <link rel="stylesheet" href="/css/main.css">
  
</head>


<body>
  <div id="wrapper">
    <header>
  <div>
    <a href="/">
    
    <h1>andrew@theinternet</h1>
    </a>
    <div class="header-links">
      <a href="/SWE.html"><h2 class="header-link">Software Engineering</h2></a>
<a href="/OMSCS.html"><h2 class="header-link">OMSCS</h2></a>
<a href="/DS.html"><h2 class="header-link">Data Science</h2></a>

    </div>
  </div>
</header>
    <div class="container">
      <section id="main_content">
        <article>
  <h2>NetSec Lecture Notes - Lesson 15 - Internet-Scale Threat Analysis -- Scanning</h2>
  <h1 id="internet-scale-threat-analysis-scanning">Internet-Scale Threat Analysis: Scanning</h1>
<ul>
  <li>The internet is a large ecosystem of networks, and thus has many vulnerabilities</li>
</ul>

<h2 id="attacker-intelligence-quiz">Attacker Intelligence Quiz</h2>
<p><strong>Attackers have three phases of intelligence gathering</strong></p>
<ul>
  <li>The attacker uses the internet to obtain information on specific IP addresses.  The kind of information gathered is: OS, services, and architecture of the target system
    <ul>
      <li>Scanning</li>
    </ul>
  </li>
  <li>The attacker gathers information about a target.  The kind of information gathered is: DNS, email servers, and the IP address range
    <ul>
      <li>Footprinting</li>
    </ul>
  </li>
  <li>The attacker gathers information on network user and group names, routing tables, and simple network management protocol
    <ul>
      <li>Enumeration</li>
    </ul>
  </li>
</ul>

<h2 id="internet-wide-security-scanning">Internet-Wide Security Scanning</h2>
<ul>
  <li>Expose new vulnerabilities</li>
  <li>Track adoption of defensive mechanisms</li>
  <li>Probing the ENTIRE address space with existing tools is both difficult and slow</li>
  <li>However the combination of a dataset of cryptographic keys plus dataset of the entire address space grants detection of vulnerabilities and understanding of the ecosystem</li>
</ul>

<h2 id="internet-wide-network-studies">Internet-Wide Network Studies</h2>
<ul>
  <li>There have been several influential studies conducted</li>
  <li>Mining Ps and Qs Widespread weak keys in network devices (2012)
    <ul>
      <li>Vulnerabilities in 5% of HTTPS hosts and 10% of SSH hosts</li>
      <li>However, took 25 hours across 25 Amazon EC2 instances (625 CPU-hours) to complete</li>
    </ul>
  </li>
  <li>EFF SSL Observatory: A glimpse at the CA ecosystem (2010)
    <ul>
      <li>3 months on 3 Linux desktop machines (6500 CPU-hours)</li>
    </ul>
  </li>
  <li>Census and Survey of the Visible Internet (2008)
    <ul>
      <li>3 months to complete ICMP census (2200 CPU-hours)</li>
    </ul>
  </li>
</ul>

<h2 id="zmap">Zmap</h2>
<ul>
  <li>What if Internet surveys didn’t require heroic efforts?</li>
  <li>What if we could scan the HTTPS ecosystem every day?</li>
  <li>What if we wrote a whole-Internet scanner from scratch?</li>
  <li>Zmap!
    <ul>
      <li>Open source tool</li>
      <li>Can port scan the entire IPv4 address space</li>
      <li>Can do so in under 45 minutes on a gigabit network at 97% of the speed of gigabit Ethernet with 98% coverage</li>
      <li><img src="../assets/content_images/omscs/netsec/L15_img1.png" alt="img" /></li>
    </ul>
  </li>
  <li>Architecture</li>
  <li>Existing network scanners: Reduce state by scanning in batches. Time lost due to blocking, results lost due to timeouts
    <ul>
      <li>Zmap: Eliminate local per-connection state.  Full asynchronous components.  No blocking except for network</li>
    </ul>
  </li>
  <li>Existing network scanners: Track individual hosts and retransmit.  Most hosts will not respond.
    <ul>
      <li>Zmap: Shotgun scanning approach.  Always send <code class="highlighter-rouge">n</code> probes per host</li>
    </ul>
  </li>
  <li>Existing network scanners: Avoid flooding through timing.  Time lost waiting.
    <ul>
      <li>Zmap: Scan widely dispersed targets.  Send as fast as network allows.</li>
    </ul>
  </li>
  <li>Existing network scanners: Utilize existing OS network stack.  Not optimized for immense number of connections.
    <ul>
      <li>Zmap: Probe-optimized network stack.  Bypass inefficiencies by generating Ethernet frames.</li>
    </ul>
  </li>
</ul>

<h2 id="addressing-probes">Addressing Probes</h2>
<ul>
  <li>How do we randomly scan addresses without excessive state?</li>
  <li>If we simply scan addresses in numerical order, we risk overloading our destination.</li>
  <li>However we also don’t want to waste time tracking what has been vs still needs to be scanned.</li>
  <li>Scan hosts according to  random permutation
    <ul>
      <li>Iterate over multiplicative group of integers modulo p, a prime slightly larger than 2<sup>32</sup></li>
      <li><img src="../assets/content_images/omscs/netsec/L15_img2.png" alt="img" /></li>
    </ul>
  </li>
  <li>How do we randomly scan addresses without excessive state?
    <ul>
      <li>Select a fresh random permutation of the address space for each scan</li>
      <li>Generate a primitive root, or a generator, of the multiplicative group</li>
      <li>Only state needed, then, is:
        <ul>
          <li>Primitive root</li>
          <li>Current address being scanned</li>
          <li>First address scanned so we know when we’re back to the beginning</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="tcp-ip-quiz">TCP IP Quiz</h2>
<ul>
  <li>Which protocol is used to break data into packets?
    <ul>
      <li>TCP</li>
    </ul>
  </li>
  <li>Which protocol is used to move packets from router to router?
    <ul>
      <li>IP</li>
    </ul>
  </li>
  <li>Which protocol reassembles the data packets?
    <ul>
      <li>TCP</li>
    </ul>
  </li>
</ul>

<h2 id="validating-responses">Validating Responses</h2>
<ul>
  <li>How do we validate responses without local per-target state?</li>
  <li>Encode secrets into mutable fields of probe packets that will have recognizable effect on responses
    <ul>
      <li>Similar to SYN cookies</li>
      <li>We know that port and sequence will be included in response, so add encoded information to those to allow analysis of who’s responding</li>
      <li><img src="../assets/content_images/omscs/netsec/L15_img3.png" alt="img" /></li>
      <li><img src="../assets/content_images/omscs/netsec/L15_img4.png" alt="img" /></li>
    </ul>
  </li>
</ul>

<h2 id="scan-rate">Scan Rate</h2>
<ul>
  <li>How fast is too fast?</li>
  <li>Researches conducted trials.  Done by configuring NICs to adjust rate.</li>
  <li>They found no correlation between hit-rate and scan-rate</li>
  <li>This leads to the conclusion that slower scanning does not reveal additional hosts</li>
</ul>

<h2 id="coverage">Coverage</h2>
<ul>
  <li>Is one probe packet sufficient?</li>
  <li>Researches conducted trials by sending multiple packets to samples of the IPv4 address space.</li>
  <li><img src="../assets/content_images/omscs/netsec/L15_img5.png" alt="img" /></li>
  <li>A plateau can be observed after about 8 packets</li>
  <li>We expect an eventual plateau in responsive hosts,  regardless of additional probes</li>
  <li>Trial provide coverage estimates:
    <ul>
      <li>1 packet: 97.9%</li>
      <li>2 packets: 98.8%</li>
      <li>3 packets: 99.4%</li>
    </ul>
  </li>
</ul>

<h2 id="zmap-comparison-with-nmap">Zmap Comparison With Nmap</h2>
<ul>
  <li>Researches conducted a study, against 1 million addresses</li>
  <li><img src="../assets/content_images/omscs/netsec/L15_img6.png" alt="img" /></li>
  <li>Zmap is capable of scanning more than 1300 times faster than the most aggressive Nmap default configuration (“insane” template)</li>
  <li>Surprisingly, Zmap also finds more results than Nmap</li>
  <li>Nmap is optimized for very different use cases than Zmap</li>
</ul>

<h3 id="why-does-zmap-find-more-hosts-than-nmap">Why does Zmap find more hosts than Nmap?</h3>
<ul>
  <li>Nmap times out hosts, after 250ms if one-packet scan, after 500ms if  two-packet scan</li>
  <li>Response times have a long tail that nmap misses
    <ul>
      <li>250ms: &lt;85%</li>
      <li>500ms: 98.2%</li>
      <li>1.0s: 99.0%</li>
      <li>8.2s: 99.9%</li>
    </ul>
  </li>
  <li>Thus, statelesness leads to both higher performance <strong>and</strong> increased coverage</li>
</ul>

<h2 id="entropy-quiz">Entropy Quiz</h2>
<ul>
  <li>With regards to computing,  what is entropy?
    <ul>
      <li>Randomness for use in cryptography or other applications that require random data</li>
    </ul>
  </li>
  <li>What are the two sources of entropy?
    <ul>
      <li>Hardware sources, and randomness generators</li>
    </ul>
  </li>
  <li>A lack of entropy will have a <strong>negative</strong> impact on performance and security?</li>
</ul>

<h2 id="cryptographic-keys">Cryptographic Keys</h2>
<ul>
  <li>Uncovering weak cryptographic keys and poor entropy collection</li>
  <li>
    <p>Research results:
<img src="../assets/content_images/omscs/netsec/L15_img7.png" alt="img" /></p>
  </li>
  <li>Why are a large number of hosts sharing cryptographic keys?</li>
  <li>5.6% of TLS hosts and 9.6% of SSH hosts share keys in a vulnerable manner
    <ul>
      <li>Default certificates and keys</li>
      <li>Apparent entropy problems</li>
    </ul>
  </li>
</ul>

<h3 id="factoring-rsa-public-keys">Factoring RSA Public Keys</h3>
<ul>
  <li>What else could go wrong if devices aren’t collecting entropy?</li>
  <li>RSA Public Key: <code class="highlighter-rouge">n = p * q</code>, <code class="highlighter-rouge">p</code> and <code class="highlighter-rouge">q</code> are two large, random primes</li>
  <li>Most efficient known method of compromising an RSA key is to factor <code class="highlighter-rouge">n</code> back to <code class="highlighter-rouge">p</code> and <code class="highlighter-rouge">q</code>
    <ul>
      <li>Ideally, with large and random primes, this is prohibitively inefficient.  Poor implementations remove this protection</li>
      <li>For example, given  <code class="highlighter-rouge">N</code><sub><code class="highlighter-rouge">1</code></sub><code class="highlighter-rouge">= p*q</code><sub><code class="highlighter-rouge">1</code></sub> and <code class="highlighter-rouge">N</code><sub><code class="highlighter-rouge">2</code></sub> <code class="highlighter-rouge">= p*q</code><sub><code class="highlighter-rouge">2</code></sub>, we can trivially compute <code class="highlighter-rouge">p = GCD(N</code><sub><code class="highlighter-rouge">1</code></sub>,<code class="highlighter-rouge">N</code><sub><code class="highlighter-rouge">2</code></sub><code class="highlighter-rouge">)</code></li>
      <li>This scenario can arrise when machines to not have sufficient entropy available, and so may generate the same prime number <code class="highlighter-rouge">p</code></li>
    </ul>
  </li>
  <li>Researchers were able to use efficient GCD calculation to crack lage numbers of RSA private keys
    <ul>
      <li>Found 2,134 distinct primes and compute the RSA keys for 64,081(0.5%)  of TLS hosts</li>
      <li>Using a similar approach for DSA, they were able to compute the private keys for 105,728 (1.03%) of SSH hosts</li>
      <li>Identified devices from &gt;40 manufacturers</li>
    </ul>
  </li>
</ul>

<h2 id="embedded-systems">Embedded Systems</h2>
<ul>
  <li>Why do so many systems generate broken keys?</li>
  <li>Linux <code class="highlighter-rouge">/dev/urandom</code>.  Used by nearly everything</li>
  <li><img src="../assets/content_images/omscs/netsec/L15_img8.png" alt="img" /></li>
  <li>Problem 1: Embedded devices may lack all these sources of entropy</li>
  <li>Problem 2: <code class="highlighter-rouge">/dev/urandom</code> can take a long time to warm up
    <ul>
      <li>Entropy is only mixed from input pool to non-blocking pool if input pool has collected more than 192 bits</li>
      <li><img src="../assets/content_images/omscs/netsec/L15_img9.png" alt="img" /></li>
      <li>Known as “Boot-Time Entropy Hole”</li>
    </ul>
  </li>
</ul>

<h2 id="certificate-authorities">Certificate Authorities</h2>
<ul>
  <li>Nearly all secure web communication uses HTTPS
    <ul>
      <li>Online banking, e-commerce, email, etc</li>
    </ul>
  </li>
  <li>HTTPS is dependent on a supporting PKI composed of “certificate authorities”, which vouch for websites’ identities</li>
  <li>Every certificate authority can sign for <em>any</em> website</li>
  <li>There is no central repository of certificate authorities
    <ul>
      <li>We don’t know who we trust until we see CAs in the wild</li>
    </ul>
  </li>
</ul>

<h2 id="certificate-chains">Certificate Chains</h2>
<ul>
  <li>Browsers decide who they trust (root CAs)</li>
  <li>These root CAs then decide to trust additional CAs.  And so on ad infinitum</li>
  <li>How do we regularly collect certificates from the Internet?</li>
  <li>Researchers conducted &gt;200 scans of the ecosystem in one year
    <ul>
      <li>Identified certificate authorities
        <ul>
          <li>Collected 42 million unique certificates from 109 million unique hosts in one year!</li>
          <li>Identified 1800 CA certificated belonging to 683 organizations
            <ul>
              <li>Including religious institutions, libraries, non-profits, financial institutions, governments, and hospitals</li>
              <li>More than 80% of organizations controlling a CA certificate aren’t commercial certificate authorities</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Uncovered some worrisome practices</li>
      <li>More than half of the certificates were provided  by the German National Research and Education Network (DFN)</li>
      <li>All major browser roots are selling intermediates to third-party organizations without any constraints</li>
    </ul>
  </li>
  <li>Who actually signs the certificates we use on a daily basis?
    <ul>
      <li>90% of trusted certificates were:
        <ul>
          <li>signed by 5 organizations</li>
          <li>descendants of 4 roots</li>
          <li>signed by 40 intermediates</li>
        </ul>
      </li>
      <li>Symantec, GoDaddy, and Comodo control 75% of the market through acquisitions</li>
      <li>26% of trusted sites are signed by a single intermediate certificate!</li>
    </ul>
  </li>
</ul>

<h2 id="ca-risks">CA Risks</h2>
<ul>
  <li>CAs are ignoring foundational principles such as defence in depth and the principle of least privilege
    <ul>
      <li>We classically teach concepts such as defense in depth, and the principle of least privilege</li>
      <li>We have methods of (and damn good reasons for) constraining what CAs can sign for, yet all but 7 of the 1,800 CA certs found can sign for anything</li>
      <li>Lack of constraints allowed a rogue CA certificate in 2012, but in another case prevented 1,400 invalid certificates</li>
      <li>Almost 5% of certificates include local domains e.g. localhost, mail, exchange</li>
    </ul>
  </li>
  <li>CAs are offering services that put the ecosystem as a whole at risk</li>
  <li>CAs are failing to recognize cryptographic reality
    <ul>
      <li>90% of certificates use a 2048 or 4096-bit RSA key</li>
      <li>50% of certificates are rooted in a 1024-bit key</li>
      <li>More than 70% of these will expire after 2016 (dated)</li>
      <li>Still signing certifcates using MD5!</li>
    </ul>
  </li>
  <li>Correctly deploying HTTPS remains difficult (this is dated, probably due to the time the course was recorded)
    <ul>
      <li>Shows some dated metrics about HTTPS adoption (2012-2013 trends).  LetsEncrypt was really a game-changer huh.</li>
    </ul>
  </li>
</ul>

</article>
      </section>
    </div>
  </div>

   
  
  <footer>
  <a href="https://creativecommons.org/licenses/by-nc/3.0/deed.en_US">
    <span>
        <b>Andrew Repp</b>
    </span>
    
    <span>© 2023</span>
  </a>
</footer>

</body>

</html>