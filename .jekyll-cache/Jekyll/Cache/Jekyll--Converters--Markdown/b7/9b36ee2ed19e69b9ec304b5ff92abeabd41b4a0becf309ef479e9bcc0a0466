I"ƒH<hr />
<h2 id="review-of-kernel-vs-user-level-threads">Review of kernel vs user level threads</h2>
<p><img src="../assets/content_images/omscs/gios/p2l4_img1.png" alt="" /></p>

<ul>
  <li>When converting a program to multithreaded, if you want to match user level threads to multiple kernel level threads, you don‚Äôt want to have to duplicate the whole PCB for every thread.
    <ul>
      <li>One solution is to split out the virtual address mapping (in PCB) from the stack and registers (in kernel level thread)</li>
    </ul>
  </li>
</ul>

<h2 id="thread-data-structures-at-scale">Thread Data Structures: at Scale</h2>
<ul>
  <li>Need copies of various structures for multiple threads and processes</li>
  <li>So, need to maintain relationships among them
    <ul>
      <li>User level thread (ULT) keeps track of state of all user level threads</li>
      <li>Process Control Block (PCB) keeps track of virtual address mappings and some other state for all processes</li>
      <li>Kernel level thread (KLT) keeps track of state of kernel level threads executing on behalf of each process</li>
      <li>CPU - if multiple CPUs need structure to represent CPU and maintain relationship between CPU and KLT
        <ul>
          <li>pointer to current and other threads</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="hard-and-light-process-state">Hard and Light Process State</h2>
<ul>
  <li>Hard process state is relevant for all user level threads in a process</li>
  <li>Light process state is only relevant for a subset of user level threads in a process.</li>
  <li>Store them separately
<img src="../assets/content_images/omscs/gios/p2l4_img2.png" alt="" /></li>
</ul>

<h2 id="rationale-for-data-structures">Rationale for Data Structures</h2>
<ul>
  <li>Single PCB
    <ul>
      <li>large continuous data structure</li>
      <li>private for each entity</li>
      <li>saved and restored on each context switch</li>
      <li>update for any changes</li>
      <li>Many cons
        <ul>
          <li>scalability</li>
          <li>overhead</li>
          <li>performance</li>
          <li>flexibility</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Mutiple data stuctures
    <ul>
      <li>smaller data structures</li>
      <li>easier to share</li>
      <li>on context switch only save and restore waht needs to change</li>
      <li>user-level library need only update portion of the state</li>
      <li>overall better than single PCB approach</li>
    </ul>
  </li>
</ul>

<h2 id="solaris-multithreading">Solaris Multithreading</h2>

<h3 id="user-level-thread-data-structures">User level thread data structures</h3>
<p><img src="../assets/content_images/omscs/gios/p2l4_img3.png" alt="" /></p>
<ul>
  <li>Most of these attributes are known at compile time, so we can allocate the right size and a thoughtful layout for this data</li>
  <li>Stack growth can be dangerous as it is not known at compile time, and one thread‚Äôs stack might overflow into another thread‚Äôs data
    <ul>
      <li>Debugging this is particularly nasty, as error appears when the second thread runs, even though the problem is within the first thread.</li>
      <li>Solution is a red zone that will cause faults within first thread if written into</li>
    </ul>
  </li>
</ul>

<h3 id="kernel-level-data-structures">Kernel level data structures</h3>
<ul>
  <li>Process
    <ul>
      <li>list of kernel level threads</li>
      <li>virtual address space</li>
      <li>user credntials</li>
      <li>signal handlers</li>
    </ul>
  </li>
  <li>Light Weight Process (LWP)
    <ul>
      <li>user level registers</li>
      <li>system call args</li>
      <li>resource usage info</li>
      <li>signal mask</li>
      <li>similar to ULT, but visible to kernel</li>
      <li>not needed when process not running -&gt; swappable</li>
    </ul>
  </li>
  <li>Kernel level threads (KLT)
    <ul>
      <li>kernel level registers</li>
      <li>stack pointer</li>
      <li>scheduling info</li>
      <li>pointers to associated LWP, process, CPU structures</li>
      <li>information needed even when process not running -&gt; not swappable</li>
    </ul>
  </li>
  <li>CPU
    <ul>
      <li>current thread</li>
      <li>list of KLT</li>
      <li>dispatching and interrupt handling information</li>
      <li>on SPARC - dedicated reg == current thread
<img src="../assets/content_images/omscs/gios/p2l4_img4.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h2 id="basic-thread-management-interactions">Basic Thread Management Interactions</h2>
<p><img src="../assets/content_images/omscs/gios/p2l4_img5.png" alt="" />
<img src="../assets/content_images/omscs/gios/p2l4_img6.png" alt="" /></p>
<ul>
  <li>User level library does not know what is happening in the kernel</li>
  <li>Kernel does not know what is happening in user level library</li>
  <li>System calls and special signals allow kernel and ULT library to interact and coordinate</li>
</ul>

<h2 id="thread-management-visibility-and-design">Thread Management Visibility and Design</h2>
<ul>
  <li>Kernel sees
    <ul>
      <li>KLTs</li>
      <li>CPUs</li>
      <li>KL Scheduler</li>
    </ul>
  </li>
  <li>UL Library sees
    <ul>
      <li>ULTs</li>
      <li>available KLTs</li>
    </ul>
  </li>
  <li>in M:1 models there is a lack of visibility between KLT and ULT, data structures and details hampers both when interacting.  One to one models addresses much of this</li>
</ul>

<h2 id="issues-on-multiple-cpus">Issues on Multiple CPUs</h2>
<ul>
  <li>Can have issues when one thread on one CPU needs to have impact on the state or execution of another thread on another CPU
    <ul>
      <li>For example, if Thread A needs to pre-empt Thread B across CPUs</li>
      <li>To do this you have to pass interrupts across CPUs and prompt execution of library code</li>
    </ul>
  </li>
</ul>

<h3 id="synchronization-related-issues">Synchronization Related Issues</h3>
<ul>
  <li>In multi-CPU case described above, if the critical secion is short it actually takes more time to communicate back and forth.
    <ul>
      <li>In this case don‚Äôt block, just spin on the mutex</li>
      <li>for long critical sections normal blocking is correct</li>
      <li>Mutexes that can either block or spin in this way are called adaptive mutexes</li>
    </ul>
  </li>
  <li>Destroying threads
    <ul>
      <li>Instead of destroying, you can reuse threads to avoid creation and destruction overhead</li>
      <li>To do this, when a thread exits
        <ul>
          <li>it‚Äôs put on a death row</li>
          <li>periodically destroyed by reaper thread</li>
          <li>if a request for a thread comes in before reaping, then thread structures / stacks are reused -&gt; performance gains!</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="interrupts-and-signals-intro">Interrupts and Signals Intro</h2>
<ul>
  <li>Interrupts
    <ul>
      <li>events generated externally to a CPU by components other than the CPU (IO devices, timers, other CPUs)</li>
      <li>represent some notification to the CPU that some external event has occurred</li>
      <li>determined by the physical platform</li>
      <li>appear asynchronously</li>
    </ul>
  </li>
  <li>Signals
    <ul>
      <li>events triggered by the CPU and software running on it</li>
      <li>determined by the operating system</li>
    </ul>
  </li>
  <li>Interrupts and Signals Both:
    <ul>
      <li>have a unique ID depending on the hardware or OS</li>
      <li>can be masked and disabled/suspended via corresponding mask
        <ul>
          <li>per-CPU interrupt mask</li>
          <li>per-process signal mask</li>
        </ul>
      </li>
      <li>if enabled, trigger corresponding handler
        <ul>
          <li>interrupt handler set for entire system by OS</li>
          <li>signal handlers set on per process basis, by process</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="interrupt-handling">Interrupt Handling</h3>
<ul>
  <li>Hardware maintains table of INT# indicators, which maps to OS-defined actions for each available interrupt</li>
</ul>

<h3 id="signal-handling">Signal Handling</h3>
<ul>
  <li>OS maintains a list of Signals.  Also maintains, for each process, a mapping of each signal to correct actions.</li>
  <li>Example given in lecture is SIGSEGV for illegal memory access, signal #11.</li>
  <li>Handlers/Actions
    <ul>
      <li>Default Actions - terminate, ignore, terminate and core dump, stop, continue</li>
      <li>Process installs handler
        <ul>
          <li>signal(), sigaction()</li>
          <li>for most signals, some cannot be ‚Äúcaught‚Äù</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Example Signals
    <ul>
      <li>Synchronous
        <ul>
          <li>SIGSEGV (access to protected memory)</li>
          <li>SIGFPE (divide by zero)</li>
          <li>SIGKILL (kill, id)
            <ul>
              <li>can be directed to a specific thread</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Asynchronous
        <ul>
          <li>SIGKILL (kill)</li>
          <li>SIGALARM</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="why-disable-interrupts-or-signals">Why Disable Interrupts or Signals?</h3>
<ul>
  <li>There is a problem with both, in that they‚Äôre executed within the context of the thread that was interrupted</li>
  <li>Very easy for signal handler in thread to deadlock trying for a mutex that the problem thread was already holding
    <ul>
      <li>One solution is to just disallow mutexes in the handler code
        <ul>
          <li>too restrictive</li>
        </ul>
      </li>
      <li>control interruptions by handler code
        <ul>
          <li>use interrupt/signal masks
  <img src="../assets/content_images/omscs/gios/p2l4_img7.png" alt="" /></li>
          <li>whenever an interrupt is considered, check mask and do not interrupt if disabled.  have handler run later when mask value changes</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Interrupt masks are per CPU
    <ul>
      <li>if mask disables interupt, hardware interrupt routing mechanism will not deliver interrupt to CPU</li>
    </ul>
  </li>
  <li>Signal masks are per execution context (ULT on top of KLT)
    <ul>
      <li>if mask disables signal, kernel sees mask and will not interrupt corresponding thread</li>
    </ul>
  </li>
</ul>

<h3 id="interrupts-on-multicore-systems">Interrupts on Multicore Systems</h3>
<ul>
  <li>Interrupts can be directed to any CPU that has them enabled</li>
  <li>may set interrupt on just a single core
    <ul>
      <li>avoids overheads and perturbations on all other cores.  Improves performance</li>
    </ul>
  </li>
</ul>

<h3 id="types-of-signals">Types of Signals</h3>
<ul>
  <li>One-shot signals
    <ul>
      <li>‚Äún signals pending == 1 signal pending‚Äù: only one execution of handler is performed regardless of how many signals received</li>
      <li>must be explicitly re-enables after handling routine is called</li>
    </ul>
  </li>
  <li>Real Time Signals
    <ul>
      <li>‚Äúif n signals raised, then handler is called n times‚Äù</li>
    </ul>
  </li>
</ul>

<h2 id="interrupts-as-threads">Interrupts as Threads</h2>
<p><img src="../assets/content_images/omscs/gios/p2l4_img8.png" alt="" /></p>
<ul>
  <li>one concern is that dynamic thread creation is expensive
    <ul>
      <li>decision to be made is whether to handle the interrupt on the stack of the original process or move to its own thread with its own execution context</li>
      <li>Heuristic is:
        <ul>
          <li>if handler doesn‚Äôt lock =&gt; execute on interrupted thread‚Äôs stack</li>
          <li>if handler can block =&gt; turn into real thread</li>
        </ul>
      </li>
      <li>Optimization
        <ul>
          <li>precreate and preinitialize thread structures for interrupt routines</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="interrupts-top-vs-bottom-half">Interrupts: Top vs Bottom Half</h3>
<p><img src="../assets/content_images/omscs/gios/p2l4_img9.png" alt="" /></p>
<ul>
  <li>split interrupt handling into a top half that follows restrictive non blocking tools</li>
  <li>and bottom half that is arbitrarily complex, can use all tools, and can be subjected to synchronization techniques</li>
  <li>Discussed in detail in the Solaris paper, includinc coverage of priority levels for both parts</li>
</ul>

<h3 id="performance-of-threads-as-interruprs">Performance of Threads as Interruprs</h3>
<ul>
  <li>Overall cost
    <ul>
      <li>overhead of 40 SPARC instructions per interrupt</li>
      <li>saving of 12 instructions per mutex
        <ul>
          <li>no changes in interrupt mask, level</li>
        </ul>
      </li>
      <li>fewer interrupts than mutex/lock unlock operations</li>
      <li>overall a performance win</li>
      <li>Optimize for the common case!
        <ul>
          <li>Common case here is mutex lock/unlock, so focus on those and be willing to pay a price elsewhere</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="threads-and-signal-handling">Threads and Signal Handling</h2>
<ul>
  <li>ULT has a mask associated with it, visible to ULL</li>
  <li>KLT has a mask associated with LWP it‚Äôs attached to, visible to Kernel</li>
  <li>Can have masks on KLT and ULT disagree with each other
    <ul>
      <li>Need a policy solution to avoid syscalls every time ULT mask is changed</li>
      <li>Proposed in Solaris papers</li>
    </ul>
  </li>
</ul>

<h3 id="case-1">Case 1</h3>
<ul>
  <li>If KLT and ULT signal masks both enabled and signal happens
    <ul>
      <li>Kernel sees that the signal is enabled</li>
      <li>Interrupts currenlty running ULT</li>
      <li>No problem, as ULT had signal enabled also</li>
    </ul>
  </li>
</ul>

<h3 id="case-2">Case 2</h3>
<ul>
  <li>If KLT has mask enabled, ULT 1 has mask disabled, and another ULT 2 (not executing, just in run queue) has mask enabled
    <ul>
      <li>ULT library knows about second thread and its mask</li>
      <li>So Kernel will pass signal to threading library by specifying a signal that calls ULT library handling routine</li>
      <li>ULT library will pass signal over to ULT 2 and thus allow it to be handled</li>
    </ul>
  </li>
</ul>

<h3 id="case-3">Case 3</h3>
<ul>
  <li>If there are two KLT with mask enabled, ULT 1 has mask disabled, and ULT 2 (running on KLT 2 on another CPU) has mask enabled
    <ul>
      <li>when signal is delivered in KLT 1 the ULT library will see that ULT 2 is running on KLT 2, and send a directed signal to KLT 2</li>
      <li>Then KLT 2 will see mask enabled, pass up to ULT library, which will see ULT 2 mask enabled, and then execute signal handler</li>
    </ul>
  </li>
</ul>

<h3 id="case-4">Case 4</h3>
<ul>
  <li>If there are two KLT with mask enabled, and two ULT on separate KLT both with mask disabled
    <ul>
      <li>When signal occurs, kernel sees mask is enabled, and sends signal up to ULT library.</li>
      <li>ULT library sees that ULT 1 has mask disabled and that no other threads have mask enabled.</li>
      <li>Thus ULT library will send syscall back down to kernal to disable mask for KLT 1</li>
      <li>Kernel will find another KLT with mask enabled and try again, sending signal</li>
      <li>Process repeats, and KLT 2 mask will be set to 0.  Repeat for each available KLT</li>
      <li>If a ULT finishes its block and enables mask again, ULT Library will perform a syscall and update KLT mask to enabled again</li>
      <li>This seems inefficient at first glance, but is another example of optimizing for the common case
        <ul>
          <li>Signals are less frequent than signal mask updates</li>
          <li>System calls avoided - cheaper to update UL mask</li>
          <li>Signal handling more expensive, but it happens infrequently so it‚Äôs still an overall win</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="tasks-in-linux">Tasks in Linux</h2>
<h3 id="task-struct-in-linus">Task Struct in Linus</h3>
<ul>
  <li>main execution abstraction == Task
    <ul>
      <li>KLT</li>
    </ul>
  </li>
  <li>single threaded process has 1 task</li>
  <li>multi threaded process has many tasks, 1 per thread
<img src="../assets/content_images/omscs/gios/p2l4_img10.png" alt="" /></li>
  <li>task identifier is housed in pid, legacy reasons</li>
  <li>group of tasks identified by original task created in tgid</li>
  <li>maintains list of tasks that are part of single process</li>
  <li>Learning from previous implementations, Linux never had one contiguous PCB
    <ul>
      <li>Instead process state is represented through a collection of references to data structures
        <ul>
          <li>Referenced via pointers</li>
          <li>Makes it easy for tasks in a single process to share some portions of the address space such as files or similar</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>To create a new task, Linux supports the clone() function
    <ul>
      <li>clone(function, stack_ptr, sharing_flags, args)
        <ul>
          <li>Similar to pthread_create
  <img src="../assets/content_images/omscs/gios/p2l4_img11.png" alt="" /></li>
          <li>sharing_flags: basically if all flags are set it‚Äôs a complete copy of parent, while if no flags are set it‚Äôs almost a fork</li>
          <li>fork() is actually implemented via clone() internally</li>
          <li>fork() is very different for multi threaded vs single threaded processes
            <ul>
              <li>for single threaded we expect child process will be a full replica of parent process</li>
              <li>for multi threaded we expect the child to be a single threaded process, a replica of only portion of address space visible to parent thread</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Linux Threads Model
    <ul>
      <li>Linux uses the Native POSIX Threads Librar (NPTL) which is a 1:1 model where every ULT has a KLT to associate to
        <ul>
          <li>kernel sees each ULT info</li>
          <li>kernel traps have become much cheaper with hardware evolution</li>
          <li>more resources: memory, large range of IDs</li>
          <li>generally the performance hit for 1:1 is now a smaller problem than the complexity of M:M</li>
        </ul>
      </li>
      <li>Oldter LinxuThreads was a M:M model
        <ul>
          <li>similar issues to those described in Solaris papers</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
:ET