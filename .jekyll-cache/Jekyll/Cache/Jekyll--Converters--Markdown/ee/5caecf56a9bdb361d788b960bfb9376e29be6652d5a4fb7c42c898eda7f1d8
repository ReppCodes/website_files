I"jH<hr />
<h2 id="synchronization-constructs">Synchronization Constructs</h2>

<h3 id="limitation-of-mutexes-and-condition-variables">Limitation of mutexes and condition variables</h3>
<ul>
  <li>error prone / correctness / ease of use
    <ul>
      <li>unlock wrong mutex, signal wrong condition variable, etc</li>
    </ul>
  </li>
  <li>lack of expressive power
    <ul>
      <li>needed helper variables for access or priority control</li>
    </ul>
  </li>
  <li>low level support
    <ul>
      <li>hardware atomic instructions</li>
    </ul>
  </li>
</ul>

<h3 id="spinlocks">Spinlocks</h3>
<ul>
  <li>basic sync construct
    <ul>
      <li>like a mutex
        <ul>
          <li>mutual exclusion</li>
          <li>lock and unlock (free)</li>
          <li>block a critical section</li>
        </ul>
      </li>
      <li>BUT
        <ul>
          <li>when the lock is busy the suspended thread isn’t blocked, but instead is spinning.
            <ul>
              <li>running on cpu and repeatedly checking lock to see if it’s free</li>
              <li>burn CPU cycles until lock is free or thread is preempted</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="semaphores">Semaphores</h3>
<ul>
  <li>common sync construct in OS kernels</li>
  <li>like a traffic light: STOP and GO</li>
  <li>similar to a mutex but more generalizable</li>
  <li>Represented by an integer value</li>
  <li>on init
    <ul>
      <li>assigned a max value (positive int)</li>
      <li>on try (wait)
        <ul>
          <li>if non-zerio -&gt; decrement and proceed</li>
          <li>if zero, block and wait</li>
          <li>number of threads allowed to proceed is the maximum value (counting semaphore)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>allow us to express count related synchronization</li>
  <li>if initalized with 1, it is roughly equivalent to a mutex</li>
  <li>on exit (post)
    <ul>
      <li>increment</li>
    </ul>
  </li>
</ul>

<h3 id="posix-semaphore-api">POSIX Semaphore API</h3>
<p><img src="../assets/content_images/omscs/gios/p3l4_img1.png" alt="" /></p>

<h2 id="reader-writer-locks">Reader Writer Locks</h2>
<ul>
  <li>Syncing different types of access
    <ul>
      <li>read (never modify)
        <ul>
          <li>shared access</li>
        </ul>
      </li>
      <li>write (always modify)
        <ul>
          <li>exclusive access</li>
        </ul>
      </li>
      <li>RWlocks specify the type of access, then locks behave accordingly
<img src="../assets/content_images/omscs/gios/p3l4_img2.png" alt="" /></li>
    </ul>
  </li>
  <li>rwlock support in Windows, Java, POSIX
    <ul>
      <li>read/write == shared/exclusive</li>
      <li>Semantic differences:
        <ul>
          <li>recursive read.lock -&gt; differs between implementations on what happens on read.unlock
            <ul>
              <li>in some cases it may unlock every single recursively locked rwlock</li>
              <li>in others, a separate read.unlock is required for each</li>
            </ul>
          </li>
          <li>upgrade/downgrade priority?
            <ul>
              <li>in some cases a reader may be given priority to upgrade the lock to write access, compared to newly arriving requests for a write lock.</li>
            </ul>
          </li>
          <li>interaction with scheduling policy
            <ul>
              <li>e.g. may block a reader if higher priority write waiting</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="monitors">Monitors</h2>
<ul>
  <li>One problem with constructs so far is that they require devs to pay attention to the pairwise operations lock/unlock wait/post etc</li>
  <li>Monitors specify
    <ul>
      <li>what is the shared resource being protected by the sync construct</li>
      <li>what are all possible entry procedures to the resource</li>
      <li>explicitly specify any relevant condition variables</li>
    </ul>
  </li>
  <li>On entry
    <ul>
      <li>lock, check, etc all handled when thread is entering the monitor</li>
    </ul>
  </li>
  <li>On exit
    <ul>
      <li>unlock, check, signal all handled when thread is exiting the monitor</li>
    </ul>
  </li>
  <li>Monitors are higher level construct
    <ul>
      <li>included in MESA by XEROX PARC</li>
      <li>Java
        <ul>
          <li>syncrhonized methods generate monitor code</li>
          <li>when compiled the resulting code will include all appropriate locking and checking.  only notify() must be done explicitly</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Monitoring also refers to the programming style that uses mutexes and condvars to describe entry and exit codes from critical section (as described in unorthodox example in enter/exit critical section material from earlier lecture)</li>
</ul>

<h2 id="more-synchronization-construct">More Synchronization Construct</h2>
<ul>
  <li>Serializers
    <ul>
      <li>make it easier to define priorities</li>
      <li>hide the need for explicit signaling and use of condvars</li>
    </ul>
  </li>
  <li>Path expressions
    <ul>
      <li>requires the programmer specify the regex that captures the correct sync behavior</li>
      <li>instead of locks or other such, programmer would specifiy e.g. “many reads OR single write” and runtime will enforce this</li>
    </ul>
  </li>
  <li>Barriers
    <ul>
      <li>inverse of semaphore</li>
      <li>blocks all threads until N threads arrive at the given point in execution</li>
    </ul>
  </li>
  <li>Rendezvous points
    <ul>
      <li>also a sync construct that waits for multiple threads to meet that particular point in execution</li>
    </ul>
  </li>
  <li>Optimistic wait-free sync (RCU)
    <ul>
      <li>efforts to achieve concurrency without explicitly locking and waiting</li>
      <li>bet on the fact that there won’t be any conflict due to concurrent writes and it’s safe to allow reads to proceed concurrently</li>
      <li>Read Copy Update lock that’s part of Linux kernel is a good example</li>
    </ul>
  </li>
  <li>All above methods need hardware support to atomically make updates to a memory location
    <ul>
      <li>this will now be discussed in detail</li>
    </ul>
  </li>
</ul>

<h2 id="sync-building-block-spinlock">Sync Building Block: Spinlock</h2>
<ul>
  <li>basic sync construct, used in creating more complex constructs
    <ul>
      <li>Using Anderson paper to explore them further</li>
      <li>Alternative implementations of spinlocks</li>
      <li>techniques generalize to other situations (e.g. atomics)</li>
    </ul>
  </li>
  <li>spinlock requires hardware support
    <ul>
      <li>to ensure that checking of lock value and setting of lock value happen atomically</li>
      <li>problem is that check + update takes multiple cycles, and concurrent check/update on different CPUs can overlap</li>
      <li>hardware-supported atomic instructions!</li>
    </ul>
  </li>
</ul>

<h3 id="atomic-instructions">Atomic Instructions</h3>
<ul>
  <li>Hardware-spedific
    <ul>
      <li>not all hardware supports the same list
        <ul>
          <li>stuck specializing/optimizing to available atomics</li>
          <li>different atomics may peform better/worse on different architectures</li>
        </ul>
      </li>
      <li>test_and_set
        <ul>
          <li>automatically returns (tests) original value and sets new value = 1 (busy)</li>
          <li>with multiple threads contending for this operation
            <ul>
              <li>first thread: test_and_set(lock)=&gt;0: free</li>
              <li>subsequent threads: test_and_set(lock)=&gt;1: busy
                <ul>
                  <li>they will reset lock to 1(busy) but that’s ok</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>read_and_increment</li>
      <li>compare_and_swap</li>
    </ul>
  </li>
  <li>Guarantees
    <ul>
      <li>atomicity</li>
      <li>mutual exclusion</li>
      <li>queue all concurrent instructions but one</li>
    </ul>
  </li>
  <li>atomic instructions == critical section with hardware-supported synchronization</li>
</ul>

<h3 id="shared-memory-and-multiprocessors">Shared Memory and Multiprocessors</h3>
<p><img src=":/6df6f822cc08444cba4443d47f844dfb" alt="ce51244baa538b24012cc1977cb01e0e.png" /></p>
<ul>
  <li>Called shared memory multiprocessors because the memory is available to all CPUs
    <ul>
      <li>aka symmetric multiprocessor, or SMP</li>
    </ul>
  </li>
  <li>Caches
    <ul>
      <li>hide memory latency</li>
      <li>SMP memory further away due to contention between CPUs for the shared memory</li>
    </ul>
  </li>
  <li>when a write to memory is tried, several things can happen
    <ul>
      <li>no – cache writing may not be allowed, and instead the write is to the memory module itself</li>
      <li>write-through – write to both cache and memory</li>
      <li>write-back – in some architectures, write can be applied in cache but actual update to memory applied later</li>
    </ul>
  </li>
  <li>Cache Coherence
    <ul>
      <li>how do we handle data showing up in multiple caches / memory modules</li>
      <li>In some architectures this must be dealt with in software
        <ul>
          <li>non-cache-coherent (NCC) architectures</li>
        </ul>
      </li>
      <li>Sometimes hardware has it covered, though, will align values across caches
        <ul>
          <li>cache-coherent (CC) architectures</li>
        </ul>
      </li>
      <li>two main approaches for handling cache coherence
        <ul>
          <li>write-invalidate
            <ul>
              <li>if one cpu changes value of X, hardware makes sure all other caches references to X are invalidated and will result in a cache miss and will be pushed to memory</li>
              <li>pros
                <ul>
                  <li>lower bandwidith on shared interconnect</li>
                  <li>amortize the cost of coherence traffic.  after first one causes invalidation no further handling is needed</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>write-update
            <ul>
              <li>if one cpu changes value of X, hardware updates all other references to X in other caches</li>
              <li>pros
                <ul>
                  <li>updated data available immediately</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>which one is used is determined by hardware, you don’t get a choice as the dev</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cache Coherence and Atomics
    <ul>
      <li>atomics always issued to the memory controller.</li>
      <li>this allows central entroy point where everything can be ordered and synchronized</li>
      <li>however, imposes sever performance penalties</li>
      <li>also, generates coherence traffic to update/invalidate all cached copies of a memory reference, even if no changes were made to value, to guarantee correctness</li>
      <li>Atomics on SMP systems are expensive!
        <ul>
          <li>expensive because of bus or I/C contention</li>
          <li>expensive because of cache bypass and coherence traffic</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="spinlock-performance-metrics">Spinlock Performance Metrics</h3>
<ul>
  <li>What are our objectives?
    <ul>
      <li>Reduce latency
        <ul>
          <li>defined as time to acquire a free lock</li>
          <li>ideally we want to immediately execute an atomic</li>
        </ul>
      </li>
      <li>Reduce waiting time (delay)
        <ul>
          <li>defined as time to stop spinning and acquire a lock that has been freed</li>
          <li>again, ideally immediately</li>
        </ul>
      </li>
      <li>Reduce contention
        <ul>
          <li>defined by traffic on bus or network IC</li>
          <li>again, ideally zero extra traffic generated</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="test-and-set-spinlock">Test-and-Set Spinlock</h3>
<p><img src=":/b34946de90c5442ab94f724689830e0b" alt="e00ddc75b84fd51b19e46ecbb82ce4c6.png" /></p>
<ul>
  <li>Pros
    <ul>
      <li>most systems support this, very portable</li>
      <li>minimal latency</li>
      <li>potentially minimal delay (spinning continuously on the atomic)</li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>causes a ton of contention - processors go to memory on each spin</li>
      <li>this is because it continously spins on the atomic instruction, which as discussed above must always be checked in memory</li>
      <li>let’s see how to fix this</li>
    </ul>
  </li>
  <li>Instead of testing lock itself, we could test cached copy.  if it’s free, only then would we execute the atomic and go to memory
    <ul>
      <li>sometimes called “spin on read” or “spin on cached value”</li>
      <li>slightly worse on latency and delay, but still good.</li>
      <li>for contention it’s better, but….
        <ul>
          <li>NCC architectures there’s literally no difference</li>
          <li>CC-WU it’s ok, but when lock is freed all CPUs looking at it will see it at once, and all will execute the atomic, going to memory and spiking traffic</li>
          <li>CC-WI – very bad.  Same probley as WU, but every single attempt to acquire the lock will not only generate contention, but will also generate invalidation traffic.
<img src=":/5754e726f5ee4fe79159b837b405e395" alt="2c941e663f1489f36ed58a4de100c0c5.png" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="spinlock-delay-alternatives">Spinlock “Delay” Alternatives</h3>
<p><img src=":/99f765569dc04cd790b905b9f515fd59" alt="3e2c1a7a9744f9cc2d061341ab6e8a76.png" /></p>
<ul>
  <li>delay after lock release
    <ul>
      <li>all CPUs see lock is free</li>
      <li>not all attempt to acquire it</li>
      <li>Pros
        <ul>
          <li>contention improved</li>
          <li>Latency is ok, still need a memory reference initially and atomic instruction</li>
        </ul>
      </li>
      <li>Cons
        <ul>
          <li>Delay is much worse, as we are forcing additional delay explicitly</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src=":/82391dd0d41943e6b1e4d0382885959c" alt="c8bc298e4964db0c69638221bb2d9d1f.png" /></p>
<ul>
  <li>Delay after each lock reference
    <ul>
      <li>doesn’t spin constantly</li>
      <li>works on NCC architectures</li>
      <li>but can hurt delay even more</li>
    </ul>
  </li>
  <li>How do we pick a correct “delay” to use for these alternatives?
    <ul>
      <li>Static Delay
        <ul>
          <li>based on fixed value, e.g. CPU ID</li>
          <li>pros - very simple approach</li>
          <li>cons - unnecessarily long delay under low contention (and presuambly too low under high contention if set wrong?)</li>
        </ul>
      </li>
      <li>Dynamic Delay
        <ul>
          <li>much more popular</li>
          <li>backoff-based</li>
          <li>random delay in a range that increases with percieved contention
            <ul>
              <li>sound similar to CN content on network flow control algorithms</li>
            </ul>
          </li>
          <li>percieved == failed test_and_set() operations</li>
          <li>delay after each reference will keep growing based on contention OR length of critical section.  noisy signal</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="queuing-lock">Queuing Lock</h3>
<p><img src=":/8f59bfd50b8e4710b69738a78f139401" alt="c0d5f44e975c6db79d62d9d0d32c2a76.png" /></p>
<ul>
  <li>Common problem in spinlock implementations
    <ul>
      <li>everyone tries to acquire a lock at the same time once lock is freed
        <ul>
          <li>solution: delay alternatives</li>
        </ul>
      </li>
      <li>everyone sees the lock is free at the same time
        <ul>
          <li>solution: anderson’s queueing lock!</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>array of flags with up to n elements (where n is number of processors in system)
    <ul>
      <li>every flag has one of two values
        <ul>
          <li>has-jock (hl)</li>
          <li>must-wait (mw)</li>
        </ul>
      </li>
      <li>every flag has two pointers
        <ul>
          <li>to the current lock holder</li>
          <li>the last element in the queue</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>when a new thread arrives at the lock it recieves a unique ticket, that is the current position of the thread in the lock
    <ul>
      <li>increment queue-last value is incremented</li>
      <li>thread is assigned the next position in the queue</li>
      <li>the read and increment of the queue-last pointer must be atomic</li>
    </ul>
  </li>
  <li>assigned queue[ticket] is a private lock</li>
  <li>enter critical section when you have lock
    <ul>
      <li>queue[ticket] == must_wait =&gt; spin</li>
      <li>queue[ticket] == has_lock =&gt; enter critical section</li>
    </ul>
  </li>
  <li>signal/set next lock holder on exit
    <ul>
      <li>queue[ticket+1] == has_lock</li>
    </ul>
  </li>
  <li>Donwsides
    <ul>
      <li>assumes read_and_increment atomic is available in the system</li>
      <li>O(N) size</li>
    </ul>
  </li>
</ul>

<p><img src=":/0d2e061860d94beb80df29bc66e84b72" alt="b5710106c173656d802a8ccb1d8c1802.png" /></p>
<ul>
  <li>All of the spinning happens on assorted other variables, the atomic is only used to give out tickets to new threads on arrival.</li>
  <li>Latency
    <ul>
      <li>read and increment operation takes more cycles.  latency not great</li>
    </ul>
  </li>
  <li>delay is very good – directly signal next cpu/thread to run, no broadcasting</li>
  <li>contention is much better than alternatives – no spinning on atomics.  but requires cc architecture and cache line aligned elements</li>
  <li>only 1 cpu/thread sees the lock is free and tries to acquire lock!</li>
</ul>

<h3 id="paper-review">Paper Review</h3>
<p>Morsel “30. Spinlock Performance Comparisons” covers findings paper deeply, and explain why each of the approaches above does better or wose in different loads and architectures.</p>

<p>Go through and review this in depth for exam.  Update notes here.  This is definitely exam question fodder</p>
:ET