I"ê5<hr />
<h2 id="provided-resources">Provided Resources</h2>
<p><a href="http://www.tldp.org/LDP/lpg/node21.html">SysV IPC Tutorials</a>
<a href="http://man7.org/linux/man-pages/man3/mq_notify.3.html">mg_notify() man page</a>
<a href="http://man7.org/linux/man-pages/man3/sem_wait.3.html">sem_wait() man page</a>
<a href="http://man7.org/linux/man-pages/man7/shm_overview.7.html">shm_overview man page</a></p>

<h2 id="inter-process-communication">Inter-Process Communication</h2>
<ul>
  <li>IPC == Os-supported mechanisms for interaction among processes (coordination and communiucation)
    <ul>
      <li>message passing IPC
        <ul>
          <li>e.g. sockets, pipes, message queues</li>
        </ul>
      </li>
      <li>memory based IPC
        <ul>
          <li>e.g. shared memory, mmapped files</li>
        </ul>
      </li>
      <li>higher-level semantics
        <ul>
          <li>a mechanism that supports more than simply a channel for two processes to coordinate or communicate amongst each other</li>
          <li>instead these methods prescribe some additional detail on protocols, formatting, exchange, etc</li>
          <li>e.g. files, RPC</li>
          <li>cover this more later</li>
        </ul>
      </li>
      <li>syncrhonization primitives
        <ul>
          <li>communication implies synchronization, and vice versa</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="message-based-ipc">Message Based IPC</h3>
<p><img src="../assets/content_images/omscs/gios/p3l3_img1.png" alt="" /></p>
<ul>
  <li>Message passing
    <ul>
      <li>send/recv of messages
        <ul>
          <li>OS creates and maintains a channel
            <ul>
              <li>buffer, FIFO queue</li>
            </ul>
          </li>
          <li>OS provides interface to processes - a port
            <ul>
              <li>processes can send/write and recv/read messages from a port</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>kernel required to
        <ul>
          <li>establish communication</li>
          <li>perform each IPC operation
            <ul>
              <li>send: system call + data copy</li>
              <li>recv: system call + data copy</li>
              <li>request-response requires:
                <ul>
                  <li>4x user/kernel crossings</li>
                  <li>4 data copies</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>Overhead</li>
    </ul>
  </li>
  <li>Pros
    <ul>
      <li>simplicity: kernel does channel management and synchronization</li>
    </ul>
  </li>
</ul>

<h4 id="forms-of-message-passing">Forms of Message Passing</h4>
<ul>
  <li>Pipes
    <ul>
      <li>carry byte stream between two processes</li>
      <li>e.g. connect output from one process to input of another process
        <ul>
          <li>bash pipes yo!</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Message Queues
    <ul>
      <li>carry ‚Äúmessages‚Äù among processes</li>
      <li>OS management includes priorities, scheduling of msg delivery</li>
      <li>APIs
        <ul>
          <li>SysV and POSIX</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Sockets
    <ul>
      <li>OS abstraction of ports</li>
      <li>send(), recv() == pass message buffers</li>
      <li>socket() == create kernel-level socket buffer</li>
      <li>associate necessary kernel6level processing (TCP/IP, etc.)</li>
      <li>can be used for different machines, as a channel between process and network device
        <ul>
          <li>if on the same machine, bypass full protocol stack</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="shared-memory-ipc">Shared Memory IPC</h3>
<p><img src="../assets/content_images/omscs/gios/p3l3_img2.png" alt="" /></p>
<ul>
  <li>Read and write to a shared memory region</li>
  <li>OS establishes shared chnnael between the processes
    <ul>
      <li>Physical pages mapped into virtual adress space</li>
      <li>The virtual address of Process 1 and the virtual address of Process 2 map to the same physical address
        <ul>
          <li>However, VA(P1) does not equal VA(P2)</li>
        </ul>
      </li>
      <li>Physical memory does not need to be contiguous</li>
      <li>All of this leverages existing memory management support on modern OS‚Äôs and hardware</li>
    </ul>
  </li>
  <li>Pros
    <ul>
      <li>System calls only for setup</li>
      <li>Data copies potentially reduced (but not eliminated)
        <ul>
          <li>Data must be explicitly allocated from shared memory region.  Must be copied if not naturally the case</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cons
    <ul>
      <li>explicit synchronization needed</li>
      <li>Communication protocol must be built and implemented by the developer
        <ul>
          <li>Distto management of the shared buffer</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>APIs
    <ul>
      <li>Linux supports 2:  SysV and POSIX</li>
      <li>Also can use file-based interface such as memory mapped files
        <ul>
          <li>Essentially analogous to the POSIX shared memory API</li>
        </ul>
      </li>
      <li>Android uses a form called ashmem</li>
    </ul>
  </li>
</ul>

<h4 id="copy-vs-map">Copy vs Map</h4>
<ul>
  <li>Contrast Shared Memory against Message Based IPC
    <ul>
      <li>Goal: transfer data from address space into another target address space</li>
    </ul>
  </li>
  <li>In message passing, the transfer requires that the CPU copy the data to/from port
    <ul>
      <li>This takes some number of CPU cycles to do</li>
    </ul>
  </li>
  <li>In shared memory based case CPU cycles are spent mapping memory into the shared address spaces
    <ul>
      <li>The CPU must also copy data into channel when necessary (no user/kernel switches though)</li>
      <li>Worthwhile if the channel is used many times</li>
      <li>may perform well for single use also, if the size of the data is large enough that copying it to/from ports takes longer than mapping address spaces</li>
      <li>e.g. in Windows leverage this difference for small data vs large data for local procedure calls ‚ÄúLPCs‚Äù automatically</li>
    </ul>
  </li>
</ul>

<h3 id="shared-memory-apis">Shared Memory APIs</h3>
<ul>
  <li>OS supports segments of shared memory.  Don‚Äôt necessarily correspond to contiguous physical pages</li>
  <li>Shared memory is system-wide.  There are system limits on number of segments and total size
    <ul>
      <li>Currently the limits are usually so big as to not be restrictive.  Modern hardware is great
        <ol>
          <li>CREATE - When a process requests that shared memory be created, the OS allocates the required amount of physical memory and assigns to it a unique key to identify the segment.</li>
        </ol>
      </li>
      <li>All communication about this segment will use this key to refer to it.
        <ol>
          <li>ATTACH - Using the key, the segment can be attached by a process.  This means that the OS establishes valid mappings between the virtual addresses in the process virtual address space, and the physical memory that backs the segment.</li>
        </ol>
      </li>
      <li>Multiple processes can attach to the same shared memory segment.</li>
      <li>Each process shares access to the same physical pages.</li>
      <li>Reads and writes to these pages will be visible across the processes.</li>
      <li>The shared memory segment can thus be mapped to different virtual addresses in different processes.
        <ol>
          <li>DETACH - Detaching a process means invalidating the address mappings for the virtual address region that corresponded to that segment within the process.</li>
        </ol>
      </li>
      <li>In other words the page table entries for those virtual addresses will no longer be valied</li>
      <li>Not the same as destroying a segment.  A segment may be detached and re-attached many times during its lifespan
        <ol>
          <li>DESTROY - Explicit request for destruction of segment (or reboot).  File will persist until deletion.</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h4 id="sysv-api-versions">SysV API Versions</h4>
<ol>
  <li>shmget(shmid,size,flag)
    <ul>
      <li>create or open a segment of appropriate size</li>
      <li>ftok (pathname, prg_id)
        <ul>
          <li>same args =&gt; same key (like hash function)</li>
          <li>to generate unique segment id</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>shmat(shmid, addr, flags)
    <ul>
      <li>addr (NULL -&gt; arbitrary)
        <ul>
          <li>map into user address space</li>
        </ul>
      </li>
      <li>cast addr to arbitrary type</li>
    </ul>
  </li>
  <li>shmdt(shmid)</li>
  <li>shmctl(smid, cmd, buf)
    <ul>
      <li>destroy with IPC_RMID</li>
    </ul>
  </li>
</ol>

<h4 id="posix-api-versions">POSIX API Versions</h4>
<ul>
  <li>most notable difference is file is used in place of segments
    <ul>
      <li>only exist in tmpfs file system.  not a traditional ‚Äúfile‚Äù</li>
      <li>allows OS mechanisms for files to be reused here
        <ul>
          <li>e.g. key == file descriptor</li>
          <li>e.g. open()/close()
            <ol>
              <li>shm_open()</li>
            </ol>
          </li>
        </ul>
      </li>
      <li>returns file descriptor</li>
      <li>in tmpfs
        <ol>
          <li>mmap() and unmmap()</li>
        </ol>
      </li>
      <li>mapping virtual to physical addresses</li>
      <li>same commands as memory mapping a file.  it‚Äôs just a  file!
        <ol>
          <li>shm_close()</li>
        </ol>
      </li>
      <li>can use open()/close() instead.  it‚Äôs just a file!
        <ol>
          <li>shm_unlink()</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h3 id="pthread-sync-for-ipc">Pthread Sync for IPC</h3>
<ul>
  <li>One of the attributes that is used to specify mutex and condition variable is whether that object is private to a process or shared among processes
    <ul>
      <li>PTHREAD_PROCESS_SHARED</li>
      <li>When syncing shared memory access of pthreads multithreaded processes, we can use mutexes and condvars that have been initialized with this flag</li>
      <li>However, synchronization data structures must also be shared!
        <ul>
          <li>Thus, allocated from shared memory region</li>
        </ul>
      </li>
      <li>Once allocated, the shared mutexes and condition variables can be used as normal
<img src="../assets/content_images/omscs/gios/p3l3_img3.png" alt="" /></li>
    </ul>
  </li>
</ul>

<h3 id="other-ipc-sync">Other IPC Sync</h3>
<ul>
  <li>shared memory accesses can be synced using OS provided mechanisms for IP interactions</li>
  <li>message queues
    <ul>
      <li>implement mutual exclusion via send/recv</li>
      <li>example protocol
        <ul>
          <li>p1 writes data to shmem, sends ‚Äúready‚Äù to queue</li>
          <li>p2 receives message, reads data, and sends ‚Äúok‚Äù message back</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>semaphores
    <ul>
      <li>binary semaphore can have 2 values, 0 or 1.  functions basically same as mutex
        <ul>
          <li>if value == 0 then stop/blocked</li>
          <li>if value == 1 then decrement(lock) and go/proceed</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="ipc-command-line-tools">IPC Command Line Tools</h3>
<ul>
  <li>ipcs == list all IPC facilities
    <ul>
      <li>-m displays info on shared memory IPC only</li>
    </ul>
  </li>
  <li>ipcrm == delete IPC facility
    <ul>
      <li>-m [shmid] deletes shm segment with given id</li>
    </ul>
  </li>
</ul>

<h3 id="shared-memory-design-considerations">Shared Memory Design Considerations</h3>
<ul>
  <li>Different APIs/mechanisms for synchronization</li>
  <li>OS only provides shared memory.  gets out of the way after</li>
  <li>All of the data passing and synchronization protocols are up to the programmer</li>
  <li>How many segments to use?
    <ul>
      <li>1 large segment
        <ul>
          <li>manager for allocating/freeing mem from shared segment</li>
        </ul>
      </li>
      <li>many small segments (one for each pairwise communication)
        <ul>
          <li>use pool of segments, queue of segment ids
            <ul>
              <li>pre-create to avoid overhead</li>
              <li>use queue to allow processes to pick off which one they‚Äôll use</li>
            </ul>
          </li>
          <li>communicate segment IDs among processes
            <ul>
              <li>with a pool of segments, P1 talking to P2 doesn‚Äôt necessarily know which segment to look at.  need to send message to tell each other.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>What size segments?
    <ul>
      <li>If size of data is known up front and doesn‚Äôt change, can just set segment size == data size and call it a day
        <ul>
          <li>This does limit max data size, as segment size is limited by OS</li>
        </ul>
      </li>
      <li>Segment size &lt; data size
        <ul>
          <li>Transfter data in rounds</li>
          <li>include protocol to track progress</li>
          <li>Probably cast shmem as data structure
            <ul>
              <li>actual data buffer</li>
              <li>sync construct</li>
              <li>flags to track progress</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>What if data doesn‚Äôt fit?</li>
    </ul>
  </li>
</ul>
:ET